{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Source\n",
    "The dataset used in this project was obtained from the [NER Mountains GitHub Repository](https://github.com/antonItachi/ner-mountains/tree/main). \n",
    "\n",
    "- This dataset contains annotated sentences related to mountains and their names, which are used for training a Named Entity Recognition (NER) model. \n",
    "- The dataset was downloaded directly and augmented to enhance diversity for the project.\n",
    "\n",
    "#### Data Collection\n",
    "- Data was collected by web scraping websites containing information about mountains.\n",
    "- The collected text data was preprocessed to extract and annotate mountain names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "from scripts.data_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_path = \"data/annotated_100.csv\"\n",
    "data = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First rows of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chimborazo in the ocean and even whole ranges ...</td>\n",
       "      <td>B-MOUNT O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I would rather go to the Elbrus than to the be...</td>\n",
       "      <td>O O O O O O B-MOUNT O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which do you like better, the sea or the Olympus?</td>\n",
       "      <td>O O O O O O O O O B-MOUNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Some people like the sea; others prefer the An...</td>\n",
       "      <td>O O O O O O O O B-MOUNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We watched the sun setting behind the Annapurna.</td>\n",
       "      <td>O O O O O O O B-MOUNT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  Chimborazo in the ocean and even whole ranges ...   \n",
       "1  I would rather go to the Elbrus than to the be...   \n",
       "2  Which do you like better, the sea or the Olympus?   \n",
       "3  Some people like the sea; others prefer the An...   \n",
       "4   We watched the sun setting behind the Annapurna.   \n",
       "\n",
       "                                          annotation  \n",
       "0  B-MOUNT O O O O O O O O O O O O O O O O O O O ...  \n",
       "1                        O O O O O O B-MOUNT O O O O  \n",
       "2                          O O O O O O O O O B-MOUNT  \n",
       "3                            O O O O O O O O B-MOUNT  \n",
       "4                              O O O O O O O B-MOUNT  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataset\n",
    "print(\"First rows of the dataset:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in the 'annotation' column:\n",
      "['B-MOUNT' 'O' 'I-MOUNT']\n",
      "\n",
      "Number of 'I-MOUNT' annotations: 6\n",
      "Number of 'B-MOUNT' annotations: 101\n"
     ]
    }
   ],
   "source": [
    "# Extract and display unique annotations\n",
    "print(\"\\nUnique values in the 'annotation' column:\")\n",
    "unique_annotations = data['annotation'].apply(lambda x: x.split()).explode().unique()\n",
    "print(unique_annotations)\n",
    "\n",
    "# Count occurrences of 'I-MOUNT' and 'B-MOUNT'\n",
    "i_mount_count = data['annotation'].apply(lambda x: x.split()).explode().value_counts().get('I-MOUNT', 0)\n",
    "b_mount_count = data['annotation'].apply(lambda x: x.split()).explode().value_counts().get('B-MOUNT', 0)\n",
    "\n",
    "print(f\"\\nNumber of 'I-MOUNT' annotations: {i_mount_count}\")\n",
    "print(f\"Number of 'B-MOUNT' annotations: {b_mount_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of two columns:\n",
    "\n",
    "- **Sentence**: Contains text with geographical references.\n",
    "- **Annotation**: Uses BIO format for NER, with labels like \"B-MOUNT\" (beginning of a mountain name), \"I-MOUNT\" (inside a mountain name), and \"O\" (non-entity tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values check:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentence      0\n",
       "annotation    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in the dataset\n",
    "print(\"\\nMissing values check:\")\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dataset size (number of rows and columns)\n",
    "print(\"\\nDataset size:\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 100 rows and 2 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentences and annotations for further processing\n",
    "sentences = data['sentence'].values\n",
    "annotations = data['annotation'].apply(lambda x: x.split()).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset will be augmented by adding more I-MOUNT annotations to increase the representation of \"inside\" mountain entities. This will help balance the occurrence of different entity labels and improve the model's ability to correctly identify and classify parts of mountain names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mountains and base sentences for augmentation\n",
    "mountains = [\n",
    "    \"Mount Kilimanjaro\", \"Mount McKinley\", \"Rocky Mountains\",\n",
    "    \"Blue Ridge Mountains\", \"Cascade Range\", \"Great Smoky Mountains\"\n",
    "]\n",
    "base_sentences = [\n",
    "    \"I climbed {} last year.\",\n",
    "    \"{} are breathtaking.\",\n",
    "    \"The view from {} is spectacular.\",\n",
    "    \"I recently visited {} with my friends.\",\n",
    "    \"{} is a famous destination for hikers.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate augmented data\n",
    "augmented_sentences, augmented_annotations = augment_data(\n",
    "    mountain_list=mountains,\n",
    "    base_sentences=base_sentences,\n",
    "    num_augmentations=100\n",
    ")\n",
    "\n",
    "# Combine original sentences with augmented sentences\n",
    "sentences = list(sentences) + augmented_sentences\n",
    "annotations = list(annotations) + augmented_annotations\n",
    "\n",
    "# Tokenize annotations by splitting each annotation string into a list of tokens\n",
    "tokenized_annotations = annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 160\n",
      "Number of testing samples: 40\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_sentences, test_sentences, train_annotations, test_annotations = train_test_split(\n",
    "    sentences, annotations, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Display the number of samples in the training and testing sets\n",
    "print(\"Number of training samples:\", len(train_sentences))\n",
    "print(\"Number of testing samples:\", len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and align labels for training and testing data\n",
    "train_inputs, train_labels = tokenize_and_align_labels(train_sentences, train_annotations, tokenizer)\n",
    "test_inputs, test_labels = tokenize_and_align_labels(test_sentences, test_annotations, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the number of tokens matches the number of labels for training data\n",
    "for i, labels in enumerate(train_labels):\n",
    "    tokens = train_inputs.encodings[i].tokens  # Get the tokens for the i-th sentence\n",
    "    if len(labels) != len(tokens):  # Compare number of labels with number of tokens\n",
    "        print(f\"Mismatch in training sample {i}: {len(labels)} labels vs {len(tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the number of tokens matches the number of labels for testing data\n",
    "for i, labels in enumerate(test_labels):\n",
    "    tokens = test_inputs.encodings[i].tokens  # Get the tokens for the i-th sentence\n",
    "    if len(labels) != len(tokens):  # Compare number of labels with number of tokens\n",
    "        print(f\"Mismatch in testing sample {i}: {len(labels)} labels vs {len(tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           --> -100\n",
      "Rocky           --> B-MOUNT\n",
      "Mountains       --> I-MOUNT\n",
      "is              --> O\n",
      "a               --> O\n",
      "famous          --> O\n",
      "destination     --> O\n",
      "for             --> O\n",
      "hike            --> O\n",
      "##rs            --> O\n",
      ".               --> -100\n",
      "[SEP]           --> -100\n",
      "[PAD]           --> -100\n",
      "[PAD]           --> -100\n",
      "[PAD]           --> -100\n"
     ]
    }
   ],
   "source": [
    "# Example debugging for tokenization and label alignment\n",
    "example_index = 1  # Change this index to check different examples\n",
    "\n",
    "# Get tokens and labels for the example\n",
    "tokens = train_inputs.encodings[example_index].tokens  # Use .tokens to get token list\n",
    "labels = train_labels[example_index]\n",
    "\n",
    "# Check alignment visually\n",
    "for token, label in zip(tokens[:15], labels[:15]):\n",
    "    print(f\"{token:15} --> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output shows token-level labels for a sentence, where:\n",
    "\n",
    "- \"Mount\" is labeled B-MOUNT (beginning of a mountain name).\n",
    "- \"Ki\", \"##lim\", \"##an\", \"##jar\", \"##o\" are labeled I-MOUNT (continuation of the name).\n",
    "- Other words are labeled O (outside any entity), and special tokens like \"[CLS]\" and punctuation are marked as -100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label-to-ID mapping\n",
    "label2id = {\"O\": 0, \"B-MOUNT\": 1, \"I-MOUNT\": 2}\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = NERDataset(train_inputs, train_labels, label2id)\n",
    "test_dataset = NERDataset(test_inputs, test_labels, label2id)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the inputs and labels\n",
    "torch.save({\n",
    "    'train_inputs': train_inputs,\n",
    "    'train_labels': train_labels,\n",
    "    'test_inputs': test_inputs,\n",
    "    'test_labels': test_labels\n",
    "}, 'data/preprocessed_data.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer\\\\tokenizer_config.json',\n",
       " 'tokenizer\\\\special_tokens_map.json',\n",
       " 'tokenizer\\\\vocab.txt',\n",
       " 'tokenizer\\\\added_tokens.json',\n",
       " 'tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained('tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also create a test dataset to demonstrate the trained model in the future\n",
    "\n",
    "*This text was generated using chatGPT 4o mini*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"Mount Everest is the tallest mountain.\",\n",
    "    \"I hiked in the Alps and the Pyrenees last summer.\",\n",
    "    \"Kilimanjaro is in Tanzania.\",\n",
    "    \"This sentence does not mention a mountain.\",\n",
    "    \"Denali, also known as Mount McKinley, is in Alaska.\",\n",
    "    \"The Andes are a stunning mountain range in South America.\",\n",
    "    \"I visited Table Mountain in South Africa last year.\",\n",
    "    \"Mount Fuji is an active volcano in Japan.\",\n",
    "    \"Rocky Mountains extend across the United States and Canada.\",\n",
    "    \"Mount Elbrus is the highest peak in Europe.\"\n",
    "]\n",
    "\n",
    "# Corresponding labels\n",
    "test_labels = [\n",
    "    [\"B-MOUNT\", \"I-MOUNT\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"B-MOUNT\", \"O\", \"O\", \"B-MOUNT\", \"O\", \"O\", \"O\"],\n",
    "    [\"B-MOUNT\", \"O\", \"O\", \"O\", \"O\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n",
    "    [\"B-MOUNT\", \"O\", \"O\", \"O\", \"B-MOUNT\", \"I-MOUNT\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n",
    "    [\"O\", \"B-MOUNT\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n",
    "    [\"O\", \"O\", \"B-MOUNT\", \"I-MOUNT\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n",
    "    [\"B-MOUNT\", \"I-MOUNT\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n",
    "    [\"B-MOUNT\", \"I-MOUNT\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n",
    "    [\"B-MOUNT\", \"I-MOUNT\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset saved as 'test_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "# Combine into a dataframe\n",
    "test_data = pd.DataFrame({\n",
    "    \"sentence\": test_sentences,\n",
    "    \"labels\": [\" \".join(labels) for labels in test_labels]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "test_data.to_csv(\"data/test_dataset.csv\", index=False)\n",
    "print(\"Test dataset saved as 'test_dataset.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Quantum-NER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
