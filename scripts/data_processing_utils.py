import random
import torch
from transformers import PreTrainedTokenizer
from torch.utils.data import Dataset
from typing import List, Tuple, Dict

# Define the augmentation function
def augment_data(mountain_list: List[str], base_sentences: List[str], num_augmentations: int = 100) -> Tuple[List[str], List[List[str]]]:
    """
    Augments a list of sentences by inserting random mountain names from `mountain_list` into 
    placeholders in `base_sentences`. For each augmented sentence, generates labels for named 
    entities corresponding to mountain names using the BIO (Begin, Inside, Outside) tagging scheme.
    
    Args:
        mountain_list (List[str]): A list of mountain names to be inserted into the base sentences.
        base_sentences (List[str]): A list of template sentences that include placeholders 
                                    where mountain names will be inserted.
        num_augmentations (int, optional): The number of augmented sentences to generate. Default is 100.

    Returns:
        Tuple[List[str], List[List[str]]]: 
            - A list of augmented sentences with mountain names inserted.
            - A list of corresponding BIO labels for each sentence.

    Example:
        mountain_list = ['Everest', 'Kilimanjaro', 'Alps']
        base_sentences = ["I love hiking in {}.", "The view from {} is amazing."]
        augment_data(mountain_list, base_sentences, num_augmentations=2)
    """
    augmented_sentences = []
    augmented_labels = []

    for _ in range(num_augmentations):
        # Select a random mountain name and a base sentence
        mountain = random.choice(mountain_list)
        sentence = random.choice(base_sentences).format(mountain)
        
        # Split the mountain name into tokens and generate corresponding labels
        tokens = mountain.split()
        labels = ["B-MOUNT"] + ["I-MOUNT"] * (len(tokens) - 1)
        
        # Tokenize the augmented sentence and generate labels for each token
        sentence_tokens = sentence.split()
        sentence_labels = []

        for token in sentence_tokens:
            if token in tokens:
                # Find the corresponding label for the token
                index = tokens.index(token)
                sentence_labels.append(labels[index])
            else:
                sentence_labels.append("O")  # "O" for tokens outside the entity
        
        # Add the augmented sentence and its labels to the results
        augmented_sentences.append(sentence)
        augmented_labels.append(sentence_labels)
    
    return augmented_sentences, augmented_labels

# Function to tokenize sentences and align labels
def tokenize_and_align_labels(
    sentences: List[str], 
    annotations: List[List[str]], 
    tokenizer: PreTrainedTokenizer
) -> Tuple[dict, List[List[int]]]:
    """
    Tokenizes a list of sentences and aligns the annotations (labels) to the tokenized output.
    This function ensures that labels are aligned correctly with the tokenized input, including 
    subword tokens generated by the tokenizer. Special tokens are assigned a label of -100 (ignored 
    in loss calculation).

    Args:
        sentences (List[str]): A list of sentences to be tokenized.
        annotations (List[List[str]]): A list of lists of labels corresponding to each word in the sentences.
                                      Each label list is aligned to the words of the corresponding sentence.
        tokenizer (PreTrainedTokenizer): A tokenizer from Hugging Face's Transformers library used for tokenizing sentences.

    Returns:
        Tuple[dict, List[List[int]]]:
            - tokenized_inputs (dict): A dictionary containing the tokenized sentences with padding, truncation, etc.
            - aligned_labels (List[List[int]]): A list of lists of aligned labels corresponding to the tokenized sentences.
    
    Example:
        sentences = ["Hiking on Mount Everest is amazing."]
        annotations = [["O", "B-MOUNT", "I-MOUNT", "O", "O"]]
        tokenize_and_align_labels(sentences, annotations, tokenizer)
    """
    
    # Tokenize the sentences, ensuring truncation, padding, and conversion to PyTorch tensors
    tokenized_inputs = tokenizer(
        sentences,
        truncation=True,
        padding="max_length",
        max_length=128,
        return_tensors="pt",
        is_split_into_words=False
    )

    aligned_labels = []
    for i, label in enumerate(annotations):
        # Retrieve the word_ids that map tokens back to the words
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_id = None
        label_ids = []

        for word_id in word_ids:
            if word_id is None:  # Special tokens ([CLS], [SEP], etc.)
                label_ids.append(-100)  # Special tokens should be ignored in loss calculation
            elif word_id < len(label):  # Ensure the word_id is within bounds of the original label
                if word_id != previous_word_id:  # First token of a word
                    label_ids.append(label[word_id])  # Assign the original label
                else:  # Subtokens (e.g., the continuation of a word)
                    # For subtokens, the label is the same as the first token (or "I-" for continuation)
                    sub_label = "I-MOUNT" if label[previous_word_id] == "B-MOUNT" else label[previous_word_id]
                    label_ids.append(sub_label)
            else:
                label_ids.append(-100)  # Handle edge case where word_id exceeds label list size

            previous_word_id = word_id

        aligned_labels.append(label_ids)

    return tokenized_inputs, aligned_labels


# Define a custom dataset for token classification
class NERDataset(Dataset):
    """
    Custom Dataset class for Named Entity Recognition (NER) tasks, specifically for token classification. 
    This dataset is designed to take tokenized inputs and their corresponding labels, 
    then return them in a format suitable for training with Hugging Face models.

    Args:
        inputs (Dict[str, torch.Tensor]): A dictionary containing tokenized input IDs and attention masks.
        labels (List[List[str]]): A list of lists, where each list contains the labels (as strings) 
                                  for each token in the sentence.
        label2id (Dict[str, int]): A dictionary that maps label strings to label IDs (e.g., {"O": 0, "B-MOUNT": 1}).

    Returns:
        A dictionary containing:
            - "input_ids" (torch.Tensor): The input token IDs for the sentence.
            - "attention_mask" (torch.Tensor): The attention mask for the sentence.
            - "labels" (torch.Tensor): The token-level label IDs for the sentence.
    """

    def __init__(self, inputs: Dict[str, torch.Tensor], labels: List[List[str]], label2id: Dict[str, int]):
        self.inputs = inputs
        self.labels = labels
        self.label2id = label2id  # Map for converting labels to IDs

    def __len__(self) -> int:
        """Return the number of examples in the dataset."""
        return len(self.inputs["input_ids"])

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """
        Get a single item from the dataset by index. The method retrieves the input IDs, attention mask, 
        and the corresponding label IDs for the given index.

        Args:
            idx (int): The index of the item to retrieve.

        Returns:
            Dict[str, torch.Tensor]: A dictionary containing:
                - "input_ids" (torch.Tensor): The input token IDs at the given index.
                - "attention_mask" (torch.Tensor): The attention mask at the given index.
                - "labels" (torch.Tensor): The label IDs at the given index.
        """
        # Convert labels to ID and handle the -100 case for ignored tokens
        label_ids = [
            self.label2id[label] if label != -100 else -100  # Convert label to ID, ignore -100
            for label in self.labels[idx]
        ]
        return {
            "input_ids": self.inputs["input_ids"][idx],
            "attention_mask": self.inputs["attention_mask"][idx],
            "labels": torch.tensor(label_ids, dtype=torch.long)  # Create tensor from label IDs
        }